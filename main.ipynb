{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains implementation of artificial neural network for classical problem of handwritten digit recognition using plain python without any neural network libraries.\n",
    "## The dataset is the MNIST dataset with 60000 training examples and 10000 test examples. Each training and test example is the grey-scaled image of 28x28 pixels.\n",
    "## The network has following tunable parameters:\n",
    "* Number of hidden layers\n",
    "* Number of neurons in eah hidden layer\n",
    "* Cost function \"Quadratic\" or \"Cross-entropy\"\n",
    "* Activation function of the output layer can be \"Sigmoid\" or \"Softmax\" (activations of hidden layers is always \"Sigmoid\")\n",
    "* Mini-batch size\n",
    "* Learning rate\n",
    "* Regularization parameter\n",
    "## The accuracy on the test set reached 97.5% in the best run with following parameters of the network:\n",
    "* Two hidden layers, 100 and 50 neurons in the first and second layers respectevely\n",
    "* Cost function \"Cross-entropy\"\n",
    "* Activation of the hidden layers \"Sigmoid\", activation of the output layer \"Softmax\"\n",
    "* Mini-batch size for stochastic gradient descent 10\n",
    "* Learning rate 0.1\n",
    "* Regularization parameter 4\n",
    "* Training over 60 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing MNIST data and converting them to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_imag_train='train-images.idx3-ubyte'\n",
    "file_label_train='train-labels.idx1-ubyte'\n",
    "file_imag_test='t10k-images.idx3-ubyte'\n",
    "file_label_test='t10k-labels.idx1-ubyte'\n",
    "data_imag=idx2numpy.convert_from_file(file_imag_train)\n",
    "data_labels=idx2numpy.convert_from_file(file_label_train)\n",
    "data_imag_test=idx2numpy.convert_from_file(file_imag_test)\n",
    "data_labels_test=idx2numpy.convert_from_file(file_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of array containing digit pixels: (60000, 28, 28)\n",
      "Shape of array containing digit labels: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of array containing digit pixels: {0}\".format(data_imag.shape))\n",
    "print(\"Shape of array containing digit labels: {0}\".format(data_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw some digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(15):\n",
    "#     plt.subplot(3,5,index+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(data_imag[index],cmap=plt.cm.gray_r,interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape 2-dim 28x28 pixels image to 1-dim 784 pixels array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imag_reshaped=np.reshape(data_imag, (data_imag.shape[0], data_imag.shape[1]*data_imag.shape[2]))\n",
    "data_imag_test_reshaped=np.reshape(data_imag_test, (data_imag_test.shape[0], data_imag_test.shape[1]*data_imag_test.shape[2]))\n",
    "inputs=data_imag_reshaped.T\n",
    "inputs_test=data_imag_test_reshaped.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_architecture=[inputs.shape[0], 100, 50, 10] # [input nodes, hidden layer1, ..., hidden layer n, output_layer]\n",
    "cost_function='CrossEntropy' # 'Quadratic' or 'CrossEntropy'\n",
    "activation_last_layer='Softmax' # 'Sigmoid' or 'Softmax'\n",
    "mini_batch_size=10 # Size of mini-batch for stochastic gradient descent approach\n",
    "learning_rate=0.1\n",
    "regularization=4\n",
    "epochs=60 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(x,y): # shuffle data for stochastic gradient descent\n",
    "    randomize=np.arange(x.shape[1])\n",
    "    np.random.shuffle(randomize)\n",
    "    x=x[:,randomize]\n",
    "    y=y[:,randomize]\n",
    "    return x, y\n",
    "def cost_fn(y_true, a, training_examples):\n",
    "    if cost_function=='Quadratic':\n",
    "        error=1/training_examples*np.sum(0.5*(a-y_true)**2)\n",
    "    elif cost_function=='CrossEntropy':\n",
    "        error=1/training_examples*np.nan_to_num(np.sum(-y_true*np.log(a)-(1-y_true)*np.log(1-a)))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cost function\")\n",
    "    return error\n",
    "def net(x, w, b): # dot product of weights and corresponding inputs\n",
    "    return np.dot(w, x)+b\n",
    "def sigmoid(x): # logistic activation function\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0, keepdims=True)\n",
    "def sigmoid_der(x): # Derivative of logistic activation function\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "# Forward propgating function in Neural network\n",
    "def forward_propagation(x,w,b, n_hidden_layers):\n",
    "    nets=[None]*(number_hidden_layers+1)\n",
    "    activations=[None]*(number_hidden_layers+1)\n",
    "    activations_der=[None]*(number_hidden_layers+1)\n",
    "    nets[0]=net(x, w[0], b[0])\n",
    "    activations[0]=sigmoid(nets[0])\n",
    "    activations_der[0]=sigmoid_der(nets[0])\n",
    "    for k in range(n_hidden_layers-1):\n",
    "        net_temp=net(activations[k], weights[k+1], bias[k+1])\n",
    "        nets[k+1]=net_temp\n",
    "        activations[k+1]=sigmoid(net_temp)\n",
    "        activations_der[k+1]=sigmoid_der(net_temp)\n",
    "    nets[-1]=net(activations[-2], weights[-1], bias[-1])\n",
    "    if activation_last_layer=='Sigmoid':\n",
    "        activations[-1]=sigmoid(nets[-1])\n",
    "        activations_der[-1]=sigmoid_der(nets[-1])\n",
    "    elif activation_last_layer=='Softmax':\n",
    "        activations[-1]=softmax(nets[-1])\n",
    "        activations_der[-1]=[]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation for the last layer\")\n",
    "    return activations, activations_der # return activations for all layers and its derivative\n",
    "# Backword propagation function in Neral network\n",
    "def backpropagation(x, y, a, a_der, n_hidden_layers):\n",
    "    dError_dWeights=[None]*(number_hidden_layers+1)\n",
    "    dError_dWeights_bias=[None]*(number_hidden_layers+1)\n",
    "    error_output=a[-1]-y\n",
    "    \n",
    "    if cost_function=='Quadratic':\n",
    "        delta=error_output*a_der[-1] # the case for mean square error cost function\n",
    "    elif cost_function=='CrossEntropy' or activation_last_layer=='Softmax':\n",
    "        delta=error_output # the case for cross entropy cost function\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cost function\")\n",
    "    dError_dWeights[-1]=np.dot(delta, a[-2].T)\n",
    "    dError_dWeights_bias[-1]=np.sum(delta, axis=1, keepdims=True)\n",
    "    for k in range(n_hidden_layers):\n",
    "        delta=np.dot(weights[-(k+1)].T, delta)*a_der[-(k+2)]\n",
    "        dError_dWeights_bias[-(k+2)]=np.sum(delta, axis=1, keepdims=True)\n",
    "        if k==n_hidden_layers-1:\n",
    "            dError_dWeights[-(k+2)]=np.dot(delta, x.T)\n",
    "        else:\n",
    "            dError_dWeights[-(k+2)]=np.dot(delta, a[-(k+3)].T)\n",
    "    return dError_dWeights, dError_dWeights_bias\n",
    " # return partial derivatives of cost function with repsect to all weights and biases for all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=inputs_test\n",
    "labels_test=data_labels_test\n",
    "labels=data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to training and validation sets (validation set is useful for hyperparameters optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labels.reshape(-1,1)\n",
    "inputs, labels=shuffle_data(inputs, labels.T)\n",
    "labels=np.squeeze(labels)\n",
    "X_train=inputs[:,:50000]\n",
    "labels_train=labels[:50000]\n",
    "X_val=inputs[:,50000:]\n",
    "labels_val=labels[50000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting labels to column vectors of size 10 (one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: [4 1 9 5 0 4 6 6 1 0]\n",
      "One hot encoded labels:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train=np.zeros((10, labels_train.shape[0]))\n",
    "y_val=np.zeros((10, labels_val.shape[0]))\n",
    "y_test=np.zeros((10, labels_test.shape[0]))\n",
    "for i in range(labels_train.shape[0]):\n",
    "    y_train[labels_train[i], i]=1\n",
    "for i in range(labels_val.shape[0]):\n",
    "    y_val[labels_val[i], i]=1\n",
    "for i in range(labels_test.shape[0]):\n",
    "    y_test[labels_test[i], i]=1\n",
    "print(\"Original labels: {0}\".format(labels_train[:10]))\n",
    "print(\"One hot encoded labels:\\n {0}\".format(y_train[:, :10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling all the pixels to unifrom range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train.T).T\n",
    "X_val = sc.transform(X_val.T).T\n",
    "X_test = sc.transform(X_test.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_training_examples=X_train.shape[1]\n",
    "number_inputs=network_architecture[0]\n",
    "number_output=network_architecture[-1]\n",
    "number_hidden_layers=len(network_architecture)-2\n",
    "number_mini_batches=number_training_examples/mini_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1 over the square root of the number of weights connecting to the same neuron.  Initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=[]\n",
    "bias=[]\n",
    "for i in range(number_hidden_layers+1):\n",
    "    weights.append(np.random.rand(network_architecture[i+1],network_architecture[i])/np.sqrt(network_architecture[i]))\n",
    "    bias.append(np.random.rand(network_architecture[i+1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main block. Here we go through epochs and within each epoch we run through all mini-batches and update weights and biases for each mini-batch. Within each epoch cost and accuracy are calculated for training, validation and test sets. Accuracies and costs are plotted. Accuracy of test set is additionally printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test set. Epoch 0: 8970 / 10000\n",
      "Accuracy for test set. Epoch 1: 9390 / 10000\n",
      "Accuracy for test set. Epoch 2: 9528 / 10000\n",
      "Accuracy for test set. Epoch 3: 9595 / 10000\n",
      "Accuracy for test set. Epoch 4: 9576 / 10000\n",
      "Accuracy for test set. Epoch 5: 9607 / 10000\n",
      "Accuracy for test set. Epoch 6: 9655 / 10000\n",
      "Accuracy for test set. Epoch 7: 9649 / 10000\n",
      "Accuracy for test set. Epoch 8: 9674 / 10000\n",
      "Accuracy for test set. Epoch 9: 9675 / 10000\n",
      "Accuracy for test set. Epoch 10: 9700 / 10000\n",
      "Accuracy for test set. Epoch 11: 9701 / 10000\n",
      "Accuracy for test set. Epoch 12: 9700 / 10000\n",
      "Accuracy for test set. Epoch 13: 9692 / 10000\n",
      "Accuracy for test set. Epoch 14: 9702 / 10000\n",
      "Accuracy for test set. Epoch 15: 9707 / 10000\n",
      "Accuracy for test set. Epoch 16: 9717 / 10000\n",
      "Accuracy for test set. Epoch 17: 9712 / 10000\n",
      "Accuracy for test set. Epoch 18: 9709 / 10000\n",
      "Accuracy for test set. Epoch 19: 9724 / 10000\n",
      "Accuracy for test set. Epoch 20: 9714 / 10000\n",
      "Accuracy for test set. Epoch 21: 9725 / 10000\n",
      "Accuracy for test set. Epoch 22: 9728 / 10000\n",
      "Accuracy for test set. Epoch 23: 9711 / 10000\n",
      "Accuracy for test set. Epoch 24: 9720 / 10000\n",
      "Accuracy for test set. Epoch 25: 9723 / 10000\n",
      "Accuracy for test set. Epoch 26: 9716 / 10000\n",
      "Accuracy for test set. Epoch 27: 9722 / 10000\n",
      "Accuracy for test set. Epoch 28: 9720 / 10000\n",
      "Accuracy for test set. Epoch 29: 9710 / 10000\n",
      "Accuracy for test set. Epoch 30: 9708 / 10000\n",
      "Accuracy for test set. Epoch 31: 9707 / 10000\n",
      "Accuracy for test set. Epoch 32: 9677 / 10000\n",
      "Accuracy for test set. Epoch 33: 9691 / 10000\n",
      "Accuracy for test set. Epoch 34: 9677 / 10000\n",
      "Accuracy for test set. Epoch 35: 9692 / 10000\n",
      "Accuracy for test set. Epoch 36: 9696 / 10000\n",
      "Accuracy for test set. Epoch 37: 9716 / 10000\n",
      "Accuracy for test set. Epoch 38: 9713 / 10000\n",
      "Accuracy for test set. Epoch 39: 9719 / 10000\n",
      "Accuracy for test set. Epoch 40: 9720 / 10000\n",
      "Accuracy for test set. Epoch 41: 9718 / 10000\n",
      "Accuracy for test set. Epoch 42: 9727 / 10000\n",
      "Accuracy for test set. Epoch 43: 9724 / 10000\n",
      "Accuracy for test set. Epoch 44: 9729 / 10000\n",
      "Accuracy for test set. Epoch 45: 9715 / 10000\n",
      "Accuracy for test set. Epoch 46: 9738 / 10000\n",
      "Accuracy for test set. Epoch 47: 9707 / 10000\n",
      "Accuracy for test set. Epoch 48: 9715 / 10000\n",
      "Accuracy for test set. Epoch 49: 9725 / 10000\n",
      "Accuracy for test set. Epoch 50: 9734 / 10000\n",
      "Accuracy for test set. Epoch 51: 9739 / 10000\n",
      "Accuracy for test set. Epoch 52: 9734 / 10000\n",
      "Accuracy for test set. Epoch 53: 9720 / 10000\n",
      "Accuracy for test set. Epoch 54: 9738 / 10000\n",
      "Accuracy for test set. Epoch 55: 9721 / 10000\n",
      "Accuracy for test set. Epoch 56: 9750 / 10000\n",
      "Accuracy for test set. Epoch 57: 9743 / 10000\n",
      "Accuracy for test set. Epoch 58: 9739 / 10000\n",
      "Accuracy for test set. Epoch 59: 9735 / 10000\n"
     ]
    }
   ],
   "source": [
    "out_test=[None]*(number_hidden_layers+1)\n",
    "cost_train=np.array([])\n",
    "cost_val=np.array([])\n",
    "cost_test=np.array([])\n",
    "accuracy_train=np.array([])\n",
    "accuracy_val=np.array([])\n",
    "accuracy_test=np.array([])\n",
    "iteration=np.array([])\n",
    "\n",
    "# Initializing graph to plot cost function and accuracy\n",
    "%matplotlib tk\n",
    "plt.ion()\n",
    "fig1, ax1=plt.subplots()\n",
    "fig2, ax2=plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Cost\")\n",
    "\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "label_added=False\n",
    "# Main loop\n",
    "for epoch in range(epochs):\n",
    "    # Splitting training set into minibatches\n",
    "    X_train_shuffled, y_train_shuffled=shuffle_data(X_train, y_train)\n",
    "    X_train_minibatches=[X_train_shuffled[:, i:i+mini_batch_size] for i in range(0, number_training_examples, mini_batch_size)]\n",
    "    y_train_minibatches=[y_train_shuffled[:, i:i+mini_batch_size] for i in range(0, number_training_examples, mini_batch_size)]\n",
    "    \n",
    "    # Updaiting weights and biases for each mini-batch\n",
    "    for i in range(len(X_train_minibatches)):            \n",
    "        # Feedforward step\n",
    "        activations, activations_der=forward_propagation(X_train_minibatches[i],weights,bias, number_hidden_layers)\n",
    "        ############################################\n",
    "        # Backpropagation step\n",
    "        dError_dWeights, dError_dWeights_bias=backpropagation(X_train_minibatches[i], y_train_minibatches[i], activations, activations_der, number_hidden_layers)\n",
    "        \n",
    "        # Updating weights using also regularization parameters\n",
    "        for k in range(number_hidden_layers+1):\n",
    "            weights[k]=(1-learning_rate*regularization/number_training_examples)*weights[k]-1/mini_batch_size*learning_rate*dError_dWeights[k]\n",
    "            #weights[k]=weights[k]-1/mini_batch_size*learning_rate*dError_dWeights[k]\n",
    "            bias[k] =bias[k]-1/mini_batch_size*learning_rate*dError_dWeights_bias[k]\n",
    "    # Predicted labels for train, validation and test sets\n",
    "    y_pred_train, temp=forward_propagation(X_train, weights, bias, number_hidden_layers)\n",
    "    y_pred_val, temp=forward_propagation(X_val, weights, bias, number_hidden_layers)\n",
    "    y_pred_test, temp=forward_propagation(X_test, weights, bias, number_hidden_layers)\n",
    "    labels_predicted_train=np.argmax(y_pred_train[-1], axis=0) # predicted digits for train set\n",
    "    labels_predicted_val=np.argmax(y_pred_val[-1], axis=0) # predicted digits for validation set\n",
    "    labels_predicted_test=np.argmax(y_pred_test[-1], axis=0) # predicted digits for test set\n",
    "    \n",
    "    # Calculating amount of correctly predicted digits\n",
    "    correctly_predicted_train=np.sum(labels_predicted_train==labels_train) \n",
    "    correctly_predicted_val=np.sum(labels_predicted_val==labels_val)\n",
    "    correctly_predicted_test=np.sum(labels_predicted_test==labels_test)\n",
    "    \n",
    "    # Plotting\n",
    "    cost_train=np.append(cost_train, cost_fn(y_train, y_pred_train[-1], len(labels_train)))\n",
    "    cost_val=np.append(cost_val, cost_fn(y_val, y_pred_val[-1], len(labels_val)))\n",
    "    cost_test=np.append(cost_test, cost_fn(y_test, y_pred_test[-1], len(labels_test)))\n",
    "    accuracy_train=np.append(accuracy_train, correctly_predicted_train/len(labels_train))\n",
    "    accuracy_val=np.append(accuracy_val, correctly_predicted_val/len(labels_val))\n",
    "    accuracy_test=np.append(accuracy_test, correctly_predicted_test/len(labels_test))\n",
    "    iteration=np.append(iteration, epoch) \n",
    "    if not label_added:\n",
    "        ax1.plot(iteration, cost_train, label=\"Train\", color='b')\n",
    "        ax1.plot(iteration, cost_val, label=\"Validation\", color='m')\n",
    "        ax1.plot(iteration, cost_test, label=\"Test\", color='g')\n",
    "        ax2.plot(iteration, accuracy_train, label=\"Train\", color='r')\n",
    "        ax2.plot(iteration, accuracy_val, label=\"Validation\", color='k')\n",
    "        ax2.plot(iteration, accuracy_test, label=\"Test\", color='c')\n",
    "        label_added=True\n",
    "    else:\n",
    "        ax1.plot(iteration, cost_train, color='b')\n",
    "        ax1.plot(iteration, cost_val, color='m')\n",
    "        ax1.plot(iteration, cost_test, color='g')\n",
    "        ax2.plot(iteration, accuracy_train, color='r')\n",
    "        ax2.plot(iteration, accuracy_val, color='k')\n",
    "        ax2.plot(iteration, accuracy_test, color='c')\n",
    "    fig2.canvas.draw()\n",
    "    fig2.canvas.flush_events()\n",
    "    fig1.canvas.draw()\n",
    "    fig1.canvas.flush_events()\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    print(\"Accuracy for test set. Epoch {0}: {1} / {2}\".format(epoch, correctly_predicted_test, len(labels_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
